{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _Merge(Layer):\n",
    "    \"\"\"Generic merge layer for elementwise merge functions.\n",
    "    Used to implement `Sum`, `Average`, etc.\n",
    "    # Arguments\n",
    "        **kwargs: standard layer keyword arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(_Merge, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_elemwise_op_output_shape(self, shape1, shape2):\n",
    "        \"\"\"Computes the shape of the resultant of an elementwise operation.\n",
    "        # Arguments\n",
    "            shape1: tuple or None. Shape of the first tensor\n",
    "            shape2: tuple or None. Shape of the second tensor\n",
    "        # Returns\n",
    "            expected output shape when an element-wise operation is\n",
    "            carried out on 2 tensors with shapes shape1 and shape2.\n",
    "            tuple or None.\n",
    "        # Raises\n",
    "            ValueError: if shape1 and shape2 are not compatible for\n",
    "                element-wise operations.\n",
    "        \"\"\"\n",
    "        if None in [shape1, shape2]:\n",
    "            return None\n",
    "        elif len(shape1) < len(shape2):\n",
    "            return self._compute_elemwise_op_output_shape(shape2, shape1)\n",
    "        elif len(shape2) == 0:\n",
    "            return shape1\n",
    "        output_shape = list(shape1[:-len(shape2)])\n",
    "        for i, j in zip(shape1[-len(shape2):], shape2):\n",
    "            if i is None or j is None:\n",
    "                output_shape.append(None)\n",
    "            elif i == 1:\n",
    "                output_shape.append(j)\n",
    "            elif j == 1:\n",
    "                output_shape.append(i)\n",
    "            else:\n",
    "                if i != j:\n",
    "                    raise ValueError('Operands could not be broadcast '\n",
    "                                     'together with shapes ' +\n",
    "                                     str(shape1) + ' ' + str(shape2))\n",
    "                output_shape.append(i)\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Used purely for shape validation.\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('A merge layer should be called '\n",
    "                             'on a list of inputs.')\n",
    "        if len(input_shape) < 2:\n",
    "            raise ValueError('A merge layer should be called '\n",
    "                             'on a list of at least 2 inputs. '\n",
    "                             'Got ' + str(len(input_shape)) + ' inputs.')\n",
    "        batch_sizes = [s[0] for s in input_shape if s is not None]\n",
    "        batch_sizes = set(batch_sizes)\n",
    "        batch_sizes -= set([None])\n",
    "        if len(batch_sizes) > 1:\n",
    "            raise ValueError('Can not merge tensors with different '\n",
    "                             'batch sizes. Got tensors with shapes : ' +\n",
    "                             str(input_shape))\n",
    "        if input_shape[0] is None:\n",
    "            output_shape = None\n",
    "        else:\n",
    "            output_shape = input_shape[0][1:]\n",
    "        for i in range(1, len(input_shape)):\n",
    "            if input_shape[i] is None:\n",
    "                shape = None\n",
    "            else:\n",
    "                shape = input_shape[i][1:]\n",
    "            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n",
    "        # If the inputs have different ranks, we have to reshape them\n",
    "        # to make them broadcastable.\n",
    "        if None not in input_shape and len(set(map(len, input_shape))) == 1:\n",
    "            self._reshape_required = False\n",
    "        else:\n",
    "            self._reshape_required = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self._reshape_required:\n",
    "            reshaped_inputs = []\n",
    "            input_ndims = list(map(K.ndim, inputs))\n",
    "            if None not in input_ndims:\n",
    "                # If ranks of all inputs are available,\n",
    "                # we simply expand each of them at axis=1\n",
    "                # until all of them have the same rank.\n",
    "                max_ndim = max(input_ndims)\n",
    "                for x in inputs:\n",
    "                    x_ndim = K.ndim(x)\n",
    "                    for _ in range(max_ndim - x_ndim):\n",
    "                        x = K.expand_dims(x, 1)\n",
    "                    reshaped_inputs.append(x)\n",
    "                return self._merge_function(reshaped_inputs)\n",
    "            else:\n",
    "                # Transpose all inputs so that batch size is the last dimension.\n",
    "                # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)\n",
    "                transposed = False\n",
    "                for x in inputs:\n",
    "                    x_ndim = K.ndim(x)\n",
    "                    if x_ndim is None:\n",
    "                        x_shape = K.shape(x)\n",
    "                        batch_size = x_shape[0]\n",
    "                        new_shape = K.concatenate([x_shape[1:], K.expand_dims(batch_size)])\n",
    "                        x_transposed = K.reshape(x, K.stack([batch_size, K.prod(x_shape[1:])]))\n",
    "                        x_transposed = K.permute_dimensions(x_transposed, (1, 0))\n",
    "                        x_transposed = K.reshape(x_transposed, new_shape)\n",
    "                        reshaped_inputs.append(x_transposed)\n",
    "                        transposed = True\n",
    "                    elif x_ndim > 1:\n",
    "                        dims = list(range(1, x_ndim)) + [0]\n",
    "                        reshaped_inputs.append(K.permute_dimensions(x, dims))\n",
    "                        transposed = True\n",
    "                    else:\n",
    "                        # We don't transpose inputs if they are 1D vectors or scalars.\n",
    "                        reshaped_inputs.append(x)\n",
    "                y = self._merge_function(reshaped_inputs)\n",
    "                y_ndim = K.ndim(y)\n",
    "                if transposed:\n",
    "                    # If inputs have been transposed, we have to transpose the output too.\n",
    "                    if y_ndim is None:\n",
    "                        y_shape = K.shape(y)\n",
    "                        y_ndim = K.shape(y_shape)[0]\n",
    "                        batch_size = y_shape[y_ndim - 1]\n",
    "                        new_shape = K.concatenate([K.expand_dims(batch_size), y_shape[:y_ndim - 1]])\n",
    "                        y = K.reshape(y, (-1, batch_size))\n",
    "                        y = K.permute_dimensions(y, (1, 0))\n",
    "                        y = K.reshape(y, new_shape)\n",
    "                    elif y_ndim > 1:\n",
    "                        dims = [y_ndim - 1] + list(range(y_ndim - 1))\n",
    "                        y = K.permute_dimensions(y, dims)\n",
    "                return y\n",
    "        else:\n",
    "            return self._merge_function(inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if input_shape[0] is None:\n",
    "            output_shape = None\n",
    "        else:\n",
    "            output_shape = input_shape[0][1:]\n",
    "        for i in range(1, len(input_shape)):\n",
    "            if input_shape[i] is None:\n",
    "                shape = None\n",
    "            else:\n",
    "                shape = input_shape[i][1:]\n",
    "            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n",
    "        batch_sizes = [s[0] for s in input_shape if s is not None]\n",
    "        batch_sizes = set(batch_sizes)\n",
    "        batch_sizes -= set([None])\n",
    "        if len(batch_sizes) == 1:\n",
    "            output_shape = (list(batch_sizes)[0],) + output_shape\n",
    "        else:\n",
    "            output_shape = (None,) + output_shape\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        if not isinstance(mask, list):\n",
    "            raise ValueError('`mask` should be a list.')\n",
    "        if not isinstance(inputs, list):\n",
    "            raise ValueError('`inputs` should be a list.')\n",
    "        if len(mask) != len(inputs):\n",
    "            raise ValueError('The lists `inputs` and `mask` '\n",
    "                             'should have the same length.')\n",
    "        if all([m is None for m in mask]):\n",
    "            return None\n",
    "        masks = [K.expand_dims(m, 0) for m in mask if m is not None]\n",
    "        return K.all(K.concatenate(masks, axis=0), axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Add(_Merge):\n",
    "    \"\"\"Layer that adds a list of inputs.\n",
    "    It takes as input a list of tensors,\n",
    "    all of the same shape, and returns\n",
    "    a single tensor (also of the same shape).\n",
    "    \"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        output = inputs[0]\n",
    "        for i in range(1, len(inputs)):\n",
    "            output += inputs[i]\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
