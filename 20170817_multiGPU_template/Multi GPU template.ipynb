{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n---------- Dirl Multi GPU template ----------\\nThere are methods to define the inputs of model\\n1. Input by queue\\n2. Input by data generator\\nyou can choose one of the methods which meets your demand,\\nwe have already commented out two blocks for each method\\n\\n\\nYou can design your own codes below the quotes ''' '''\\n\\nFor example : \\n\\n''' define your optimizer '''\\nopt = tf.train.AdamOptimizer(0.0001, beta1=0.5, beta2=0.999)\\n\\nAuthor: Shin-Yi Ding\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "---------- Dirl Multi GPU template ----------\n",
    "There are methods to define the inputs of model\n",
    "1. Input by queue\n",
    "2. Input by data generator\n",
    "you can choose one of the methods which meets your demand,\n",
    "we have already commented out two blocks for each method\n",
    "\n",
    "\n",
    "You can design your own codes below the quotes ''' '''\n",
    "\n",
    "For example : \n",
    "\n",
    "''' define your optimizer '''\n",
    "opt = tf.train.AdamOptimizer(0.0001, beta1=0.5, beta2=0.999)\n",
    "\n",
    "Author: Shin-Yi Ding\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(agl_dim=2, batch_size=128, channel=3, dataset='test_set', ef_dim=14, encoded_agl_dim=16, eye='None', gpus='0', height=41, is_cfw_only=False, lr=0.0001, steps=1250000, width=51)\n"
     ]
    }
   ],
   "source": [
    "# system package\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# defined package\n",
    "import deepwarp\n",
    "import load_dataset\n",
    "\n",
    "# get the predefined parameters\n",
    "# we implement this by package:argparse\n",
    "from config import get_config\n",
    "conf,_ = get_config()\n",
    "TOWER_NAME = 'tower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' set name (for save checkpoints) '''\n",
    "ckp_fn = 'Adam_0.001_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' set visible gpu IDs '''\n",
    "# input example in config, \"1,2\"\n",
    "gpus = conf.gpus.split(',')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=','.join([str(i) for i in gpus])\n",
    "n_gpus = len(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' define input setting '''\n",
    "# case dependent\n",
    "validation_portion = 0.05\n",
    "conf.eye = \"L\"\n",
    "data_dir = conf.dataset\n",
    "dirs = np.asarray([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "training_dirs = dirs[0:(dirs.shape[0]-int(dirs.shape[0]*validation_portion))]\n",
    "valiation_dirs = dirs[(dirs.shape[0]-int(dirs.shape[0]*validation_portion)):dirs.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define by cifar-10 example\n",
    "# you don't have to change this function\n",
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "        is over individual gradients. The inner list is over the gradient\n",
    "        calculation for each tower.\n",
    "    Returns:\n",
    "       List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "       across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    \n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        \n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "    \n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the inputs and outputs of the inference and loss function here\n",
    "# the inference model and loss function are defined in the *deepwarp.py*\n",
    "def tower_loss(scope, input_img, input_fp, input_ang, img_, phase_train):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      scope: unique prefix string identifying the tower, e.g. 'tower_0'\n",
    "      input_(): batch inputs\n",
    "    Returns:\n",
    "       Tensor of shape [] containing the total loss for a batch of data\n",
    "    \"\"\"\n",
    "\n",
    "    ''' define your inputs and outputs of inference function'''\n",
    "    img_pred = deepwarp.inference(input_img, input_fp, input_ang, phase_train, conf)\n",
    "    \n",
    "    ''' define your inputs and outputs of loss function '''\n",
    "    with tf.name_scope('l2_loss'):\n",
    "        loss = deepwarp.loss(img_pred, img_)\n",
    "   \n",
    "    # Assemble all of the losses for the current tower only.\n",
    "    losses = tf.get_collection('losses', scope)\n",
    "\n",
    "    # Calculate the total loss for the current tower.\n",
    "    total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the main function of training process, you have to design an input method here\n",
    "\n",
    "def train():\n",
    "    # calculate in cpu\n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        \n",
    "        # define the counter of global step\n",
    "        global_step = tf.get_variable('global_step',\n",
    "                                      [],\n",
    "                                      initializer=tf.constant_initializer(0),\n",
    "                                      trainable=False)\n",
    "        \n",
    "        # define the phase train constant (bool) for batch normalization\n",
    "        with tf.name_scope('phase_train'):\n",
    "            phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "  \n",
    "        ''' there are two methods to define the inputs of model '''\n",
    "        # 1. input by queue\n",
    "        # initial the data generator\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        # define your queue in *load_dataset.py\n",
    "        with tf.name_scope('create_inputs'):\n",
    "            reader = load_dataset.DataGenerator(coord,\n",
    "                                                pack_size=conf.batch_size*n_gpus,\n",
    "                                                buffer_ratio=3,\n",
    "                                                data_dir=data_dir,\n",
    "                                                input_dirs=training_dirs,\n",
    "                                                eye=conf.eye)\n",
    "        \n",
    "        # 2. input by data generator\n",
    "        # define your generator in *load_dataset.py\n",
    "        # with tf.name_scope('create_inputs'):\n",
    "        #     reader = load_dataset.load_data(data_dir, training_dirs, conf.batch_size*n_gpus, conf.eye)        \n",
    "        # define the placeholder for inputs\n",
    "        # with tf.name_scope('inputs'):\n",
    "        #     input_img = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name=\"input_img\") # [None, 41, 51, 3]\n",
    "        #     input_fp = tf.placeholder(tf.float32, [None, conf.height, conf.width,conf.ef_dim], name=\"input_fp\") # [None, 41, 51, 14]\n",
    "        #     input_ang = tf.placeholder(tf.float32, [None, conf.agl_dim], name=\"input_ang\") ## [None, 41, 51, 2]\n",
    "        #     img_ = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name =\"Ground_Truth\")\n",
    "\n",
    "        \n",
    "        ''' define the optimizer '''\n",
    "        opt = tf.train.AdamOptimizer(conf.lr, beta1=0.5, beta2=0.999)\n",
    "             \n",
    "        # initial the tower gradients        \n",
    "        tower_grads = []\n",
    "        \n",
    "        # define the multiple gpu towers\n",
    "        # you only need to deal with the input here\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in range(n_gpus):\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % (TOWER_NAME, i)) as scope:\n",
    "                                                \n",
    "                        ''' there are two methods to feed the training inputs '''\n",
    "                        # 1. calculate loss (feed input by queue)\n",
    "                        # get training batch from queue\n",
    "                        input_batch = reader.dequeue(conf.batch_size)\n",
    "                        \n",
    "                        # calculate the loss for each tower\n",
    "                        loss = tower_loss(scope,\n",
    "                                          input_batch[0],\n",
    "                                          input_batch[1],\n",
    "                                          input_batch[2],\n",
    "                                          input_batch[3],\n",
    "                                          phase_train)\n",
    "                        # 2. calculate loss (feed input by feed_dict)\n",
    "                        # loss = tower_loss(scope,\n",
    "                        #                   input_img[i*conf.batch_size:(i+1)*conf.batch_size],\n",
    "                        #                   input_fp[i*conf.batch_size:(i+1)*conf.batch_size],\n",
    "                        #                   input_ang[i*conf.batch_size:(i+1)*conf.batch_size],\n",
    "                        #                   img_[i*conf.batch_size:(i+1)*conf.batch_size],\n",
    "                        #                   phase_train)\n",
    "                        \n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "                                                \n",
    "                        # Calculate the gradients for the batch of data on this tower.\n",
    "                        grads = opt.compute_gradients(loss)\n",
    "                        \n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "        \n",
    "        # calculate mean gradients of towers\n",
    "        grads = average_gradients(tower_grads)\n",
    "               \n",
    "        # apply gradients\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "        \n",
    "        # define sess\n",
    "        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "        ''' start queue runners (Only needed when you input the dataset with queue) '''\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        reader.start_threads(sess, n_threads=2)\n",
    "    \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for s in range(conf.steps):\n",
    "            start_time = time.time()\n",
    "                        \n",
    "            ''' define your sess.run  (feed input by queue) '''\n",
    "            # 1. define by queue\n",
    "            _, loss_train = sess.run([apply_gradient_op, loss], feed_dict={phase_train: True})\n",
    "\n",
    "            # 2.define by feed_dict\n",
    "            # inputs_batch = next(reader)\n",
    "            # _, loss_train = sess.run([apply_gradient_op, loss], feed_dict={phase_train: True,\n",
    "            #                                                                input_img: inputs_batch[0],\n",
    "            #                                                                input_fp:inputs_batch[1],\n",
    "            #                                                                input_ang:inputs_batch[2],\n",
    "            #                                                                img_:inputs_batch[3]})\n",
    "            \n",
    "            # print loss and time cost\n",
    "            if (s % 100 == 0):\n",
    "                duration = time.time() - start_time    \n",
    "                start_time = time.time()\n",
    "                print('Step %d, loss = %.4f (%.2f s)' % (s, np.mean(loss_train), duration))\n",
    "                \n",
    "            # save checkpoint\n",
    "            if s%1000 == 0:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, \"checkpoints/\"+ ckp_fn +\"_\"+conf.dataset+'_'+conf.eye, global_step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 0.3101 (2.21 s)\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
