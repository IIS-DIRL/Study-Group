{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(agl_dim=2, batch_size=128, channel=3, dataset='None', ef_dim=14, encoded_agl_dim=16, epochs=500, eye='None', height=41, is_cfw_only=False, load_weights='None', lr=0.0001, width=51)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import xrange\n",
    "import deepwarp\n",
    "import load_dataset2\n",
    "\n",
    "from config import get_config\n",
    "conf,_ = get_config()\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', '/log_pred', \"\"\"Directory where to write event logs.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '/checkpoints', \"\"\"Directory where to read model checkpoints.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5, \"\"\"How often to run the eval.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_examples', 10000, \"\"\"Number of examples to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('run_once', False, \"\"\"Whether to run eval only once.\"\"\")\n",
    "conf.dataset = 'dirl_v2'\n",
    "tf.app.flags.DEFINE_string('data_dir', '../../dataset/', \"\"\"Path to the dataset directory.\"\"\")\n",
    "validation_portion=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def eval_once(saver, summary_writer, top_k_op, summary_op):\n",
    "#     \"\"\"Run Eval once.\n",
    "#     Args:\n",
    "#       saver: Saver.\n",
    "#       summary_writer: Summary writer.\n",
    "#       top_k_op: Top K op.\n",
    "#       summary_op: Summary op.\n",
    "#     \"\"\"\n",
    "#     with tf.Session() as sess:\n",
    "#         ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "#         if ckpt and ckpt.model_checkpoint_path:\n",
    "#             # Restores from checkpoint\n",
    "#             saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#             # Assuming model_checkpoint_path looks something like:\n",
    "#             #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "#             # extract global_step from it.\n",
    "#             global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "#         else:\n",
    "#             print('No checkpoint file found')\n",
    "#             return\n",
    "\n",
    "#         # Start the queue runners.\n",
    "#         coord = tf.train.Coordinator()\n",
    "#         try:\n",
    "#             threads = []\n",
    "#             for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "#                 threads.extend(qr.create_threads(sess, coord=coord, daemon=True, start=True))\n",
    "\n",
    "#             num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "#             true_count = 0  # Counts the number of correct predictions.\n",
    "#             total_sample_count = num_iter * FLAGS.batch_size\n",
    "#             step = 0\n",
    "#             while step < num_iter and not coord.should_stop():\n",
    "#                 predictions = sess.run([top_k_op])\n",
    "#                 true_count += np.sum(predictions)\n",
    "#                 step += 1\n",
    "\n",
    "#             # Compute precision @ 1.\n",
    "#             precision = true_count / total_sample_count\n",
    "#             print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "#             summary = tf.Summary()\n",
    "#             summary.ParseFromString(sess.run(summary_op))\n",
    "#             summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "#             summary_writer.add_summary(summary, global_step)\n",
    "#         except Exception as e:  # pylint: disable=broad-except\n",
    "#             coord.request_stop(e)\n",
    "\n",
    "#         coord.request_stop()\n",
    "#         coord.join(threads, stop_grace_period_secs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pair(imgs):\n",
    "    for uid in xrange(len(imgs)):\n",
    "        # print(imgs[uid].shape)\n",
    "        n_img = np.arange(len(imgs[uid]))\n",
    "        sur, tar = np.meshgrid(n_img, n_img)\n",
    "        if uid == 0:\n",
    "            pairs = np.concatenate((np.expand_dims(np.repeat(uid, len(imgs[uid])*len(imgs[uid])), axis = 1),\n",
    "                                    np.expand_dims(np.reshape(sur,-1), axis = 1),\n",
    "                                    np.expand_dims(np.reshape(tar,-1), axis = 1)), axis = 1)\n",
    "        else:\n",
    "            pairs = np.concatenate((pairs, np.concatenate((np.expand_dims(np.repeat(uid, len(imgs[uid])*len(imgs[uid])), axis = 1),\n",
    "                                                           np.expand_dims(np.reshape(sur,-1), axis = 1),\n",
    "                                                            np.expand_dims(np.reshape(tar,-1), axis = 1)), axis = 1)),\n",
    "                                  axis = 0)\n",
    "    print(pairs.shape)\n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iterator(imgs, agls, anchor_maps, pairs, batch_size):\n",
    "    batch_idx = 0\n",
    "    while True:\n",
    "        idxs = np.arange(0, len(pairs))\n",
    "        np.random.shuffle(idxs)\n",
    "        for batch_idx in range(0, len(idxs), batch_size):\n",
    "            cur_idxs = idxs[batch_idx:batch_idx+batch_size]\n",
    "            pairs_batch = pairs[cur_idxs]\n",
    "            img_batch = []\n",
    "            fp_batch = []\n",
    "            agl_batch = []\n",
    "            img__batch = []\n",
    "            for pair_idx in range(len(pairs_batch)):\n",
    "                uID = pairs_batch[pair_idx,0]\n",
    "                surID = pairs_batch[pair_idx,1]\n",
    "                tarID = pairs_batch[pair_idx,2]\n",
    "                print(uID, surID, tarID)\n",
    "                if pair_idx == 0:\n",
    "                    img_batch.append(imgs[uID][surID])\n",
    "                    agl_batch.append(agls[uID][tarID] - agls[uID][surID])\n",
    "                    fp_batch.append(anchor_maps[uID][surID])\n",
    "                    img__batch.append(imgs[uID][tarID])\n",
    "                else:\n",
    "                    img_batch.append(imgs[uID][surID])\n",
    "                    agl_batch.append(agls[uID][tarID] - agls[uID][surID])\n",
    "                    fp_batch.append(anchor_maps[uID][surID])\n",
    "                    img__batch.append(imgs[uID][tarID])\n",
    "              \n",
    "\n",
    "#             print(np.asarray(img_batch).shape)\n",
    "#             print(np.asarray(agl_batch).shape)            \n",
    "#             print(np.asarray(fp_batch).shape)\n",
    "#             print(np.asarray(img__batch).shape)\n",
    "#             print(pairs_batch.shape)\n",
    "            yield np.asarray(img_batch), np.asarray(fp_batch), np.asarray(agl_batch), np.asarray(img__batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324856, 3)\n",
      "18 45 64\n",
      "21 58 18\n",
      "1 56 28\n",
      "11 32 21\n",
      "2 64 42\n",
      "9 5 42\n",
      "12 91 97\n",
      "12 8 35\n",
      "8 23 14\n",
      "7 17 69\n",
      "12 63 93\n",
      "9 59 24\n",
      "22 63 14\n",
      "19 56 16\n",
      "17 1 13\n",
      "15 94 84\n",
      "13 78 15\n",
      "23 58 73\n",
      "28 78 74\n",
      "27 52 98\n",
      "30 96 10\n",
      "25 91 19\n",
      "6 25 43\n",
      "6 68 69\n",
      "32 75 12\n",
      "4 34 17\n",
      "6 43 37\n",
      "27 11 26\n",
      "6 48 91\n",
      "14 89 83\n",
      "7 93 65\n",
      "31 36 3\n",
      "4 83 9\n",
      "5 86 17\n",
      "27 12 4\n",
      "28 20 41\n",
      "5 91 9\n",
      "4 90 88\n",
      "15 87 39\n",
      "22 57 87\n",
      "17 69 79\n",
      "15 60 48\n",
      "24 96 31\n",
      "9 8 61\n",
      "28 4 8\n",
      "1 78 3\n",
      "32 66 23\n",
      "26 92 98\n",
      "29 40 40\n",
      "17 54 13\n",
      "8 4 56\n",
      "16 40 5\n",
      "15 97 78\n",
      "6 8 32\n",
      "0 2 39\n",
      "0 12 32\n",
      "5 37 12\n",
      "18 63 5\n",
      "31 31 12\n",
      "29 69 27\n",
      "16 29 26\n",
      "3 18 14\n",
      "16 37 58\n",
      "3 42 15\n",
      "27 4 7\n",
      "13 65 61\n",
      "27 48 55\n",
      "31 35 39\n",
      "11 24 28\n",
      "19 65 4\n",
      "1 18 17\n",
      "22 19 94\n",
      "11 21 67\n",
      "23 32 65\n",
      "14 38 65\n",
      "2 47 56\n",
      "27 7 72\n",
      "12 33 97\n",
      "26 8 71\n",
      "6 68 82\n",
      "17 85 11\n",
      "30 93 68\n",
      "26 93 33\n",
      "6 51 13\n",
      "19 49 45\n",
      "27 87 22\n",
      "29 33 48\n",
      "2 13 45\n",
      "14 88 24\n",
      "24 61 79\n",
      "25 65 20\n",
      "9 89 58\n",
      "2 59 64\n",
      "12 43 3\n",
      "19 68 96\n",
      "31 65 61\n",
      "28 15 61\n",
      "25 0 19\n",
      "25 48 42\n",
      "25 34 89\n",
      "3 56 35\n",
      "13 62 2\n",
      "22 55 6\n",
      "9 40 87\n",
      "28 1 74\n",
      "0 40 31\n",
      "16 27 85\n",
      "32 14 49\n",
      "2 98 27\n",
      "2 4 62\n",
      "2 19 53\n",
      "9 51 67\n",
      "24 57 26\n",
      "15 38 66\n",
      "27 6 45\n",
      "2 50 31\n",
      "8 9 32\n",
      "11 41 24\n",
      "24 58 31\n",
      "1 65 10\n",
      "1 33 82\n",
      "28 48 88\n",
      "16 63 36\n",
      "26 94 93\n",
      "28 64 16\n",
      "3 43 91\n",
      "20 89 62\n",
      "30 48 3\n",
      "0.020032405853271484\n",
      "[[  5.  28.]\n",
      " [-13. -31.]\n",
      " [ 19. -40.]\n",
      " [-46.   8.]\n",
      " [  0. -51.]\n",
      " [-21.   2.]\n",
      " [ 23.   2.]\n",
      " [  8. -29.]\n",
      " [  5.   3.]\n",
      " [-10.  40.]\n",
      " [  4.   3.]\n",
      " [-28. -15.]\n",
      " [-12. -29.]\n",
      " [ 20. -30.]\n",
      " [ 10. -10.]\n",
      " [-16.  -2.]\n",
      " [  2. -47.]\n",
      " [ 30.  10.]\n",
      " [ 22.  -5.]\n",
      " [ 41.  -1.]\n",
      " [ 17. -21.]\n",
      " [ 10. -60.]\n",
      " [ 16. -19.]\n",
      " [-10.   0.]\n",
      " [-20. -40.]\n",
      " [  0.  10.]\n",
      " [ 10.  10.]\n",
      " [ 33. -10.]\n",
      " [ 30.  40.]\n",
      " [ 40. -10.]\n",
      " [-40. -20.]\n",
      " [-29.  -7.]\n",
      " [ 30. -42.]\n",
      " [  3. -50.]\n",
      " [ 40.  10.]\n",
      " [ 30. -20.]\n",
      " [-33. -44.]\n",
      " [ 20.   0.]\n",
      " [ 20. -80.]\n",
      " [-10.  20.]\n",
      " [-30.  10.]\n",
      " [-30. -10.]\n",
      " [-17. -70.]\n",
      " [  0.  28.]\n",
      " [-11.  -1.]\n",
      " [ -1. -37.]\n",
      " [  1. -38.]\n",
      " [ 40.   0.]\n",
      " [  0.   0.]\n",
      " [-30. -20.]\n",
      " [  0.  20.]\n",
      " [ 40.  30.]\n",
      " [-10.  21.]\n",
      " [-20. -19.]\n",
      " [-28. -30.]\n",
      " [ 17. -12.]\n",
      " [ -3.  12.]\n",
      " [ 27. -26.]\n",
      " [ 24.  15.]\n",
      " [-50. -50.]\n",
      " [ -6.   0.]\n",
      " [-60.   0.]\n",
      " [-30.  40.]\n",
      " [ 30. -20.]\n",
      " [-11.  -5.]\n",
      " [ 28.  -1.]\n",
      " [ -4.  10.]\n",
      " [-50. -10.]\n",
      " [-47.  -1.]\n",
      " [  5. -25.]\n",
      " [-10.   0.]\n",
      " [-30.  60.]\n",
      " [ 15.  40.]\n",
      " [ 16.  46.]\n",
      " [-16.  59.]\n",
      " [-22.  15.]\n",
      " [ 19.  37.]\n",
      " [ 13.  46.]\n",
      " [-18.  14.]\n",
      " [ 20.  10.]\n",
      " [  8.  -4.]\n",
      " [-30. -20.]\n",
      " [ 50. -70.]\n",
      " [ -2. -16.]\n",
      " [  4.  -7.]\n",
      " [ 18. -70.]\n",
      " [ 30.  40.]\n",
      " [ 16.  13.]\n",
      " [ 11. -66.]\n",
      " [ -1.  13.]\n",
      " [ 31. -36.]\n",
      " [ 30. -30.]\n",
      " [ 10.   1.]\n",
      " [  0. -10.]\n",
      " [  7. -12.]\n",
      " [  2.   9.]\n",
      " [ 30.  28.]\n",
      " [ 20. -10.]\n",
      " [ 18.  -4.]\n",
      " [-32.  43.]\n",
      " [ 33. -51.]\n",
      " [-15. -11.]\n",
      " [  0. -10.]\n",
      " [-18.  45.]\n",
      " [ 35.  34.]\n",
      " [-30.  10.]\n",
      " [-44.  58.]\n",
      " [ 21.  20.]\n",
      " [  4. -36.]\n",
      " [ 10.  20.]\n",
      " [  0.  20.]\n",
      " [ 10.  10.]\n",
      " [ 13. -39.]\n",
      " [ 20.  60.]\n",
      " [-38.  10.]\n",
      " [-10. -30.]\n",
      " [-22. -17.]\n",
      " [  9. -25.]\n",
      " [ 11. -40.]\n",
      " [ 19. -36.]\n",
      " [-50.  60.]\n",
      " [  3.  40.]\n",
      " [-10. -40.]\n",
      " [ 10.   0.]\n",
      " [  9. -39.]\n",
      " [ 30.  40.]\n",
      " [  7. -26.]\n",
      " [  9. -10.]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "  \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default() as g:\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    data_dir = os.path.join(FLAGS.data_dir, conf.dataset, 'trainging_pickle/')\n",
    "    dirs = np.asarray([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    # training_dirs = dirs[0:(dirs.shape[0]-int(dirs.shape[0]*validation_portion))]\n",
    "    # valiation_dirs = dirs[(dirs.shape[0]-int(dirs.shape[0]*validation_portion)):dirs.shape[0]]\n",
    "    imgs, agls, _, anchor_maps = load_dataset2.load(data_dir=data_dir, dirs = dirs, eye = \"L\", pose = \"0P\")\n",
    "    \n",
    "    if(len(imgs)!= len(agls) & len(imgs)!= len(anchor_maps)):\n",
    "        sys.exit(\"Wrong length between 3 inputs\")\n",
    "    \n",
    "    pairs = get_pair(agls)\n",
    "    iter_ = data_iterator(imgs, agls, anchor_maps, pairs, 128)\n",
    "    tcb = time.time()\n",
    "    inputs_batch = next(iter_)\n",
    "    print(time.time()-tcb)\n",
    "    print(inputs_batch[1])\n",
    "#     # define placeholder for inputs to network\n",
    "#     with tf.name_scope('inputs'):\n",
    "#         input_img = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name=\"input_img\") # [None, 41, 51, 3]\n",
    "#         input_fp = tf.placeholder(tf.float32, [None, conf.height, conf.width,conf.ef_dim], name=\"input_fp\") # [None, 41, 51, 14]\n",
    "#         input_ang = tf.placeholder(tf.float32, [None, conf.agl_dim], name=\"input_ang\") ## [None, 41, 51, 2]\n",
    "#         phase_train = tf.placeholder(tf.bool, name='phase_train') # a bool for batch_normalization\n",
    "#         img_ = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name =\"Ground_Truth\")\n",
    "#     # Build a Graph that computes the logits predictions from the\n",
    "#     # inference model.\n",
    "#     img_pred = deepwarp.inference(input_img, input_fp, input_ang, phase_train, conf)\n",
    "\n",
    "#     # Calculate predictions.ji3\n",
    "#     top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "#     # Restore the moving average version of the learned variables for eval.\n",
    "#     variable_averages = tf.train.ExponentialMovingAverage(\n",
    "#         cifar10.MOVING_AVERAGE_DECAY)\n",
    "#     variables_to_restore = variable_averages.variables_to_restore()\n",
    "#     saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "#     # Build the summary operation based on the TF collection of Summaries.\n",
    "#     summary_op = tf.summary.merge_all()\n",
    "\n",
    "#     summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "#         if ckpt and ckpt.model_checkpoint_path:\n",
    "#             # Restores from checkpoint\n",
    "#             saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#             # Assuming model_checkpoint_path looks something like:\n",
    "#             #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "#             # extract global_step from it.\n",
    "#             global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "#         else:\n",
    "#             print('No checkpoint file found')\n",
    "#             return\n",
    "\n",
    "#         # Start the queue runners.\n",
    "#         coord = tf.train.Coordinator()\n",
    "#         try:\n",
    "#             threads = []\n",
    "#             for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "#                 threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "#                                              start=True))\n",
    "\n",
    "#             num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "#             true_count = 0  # Counts the number of correct predictions.\n",
    "#             total_sample_count = num_iter * FLAGS.batch_size\n",
    "#             step = 0\n",
    "#             while step < num_iter and not coord.should_stop():\n",
    "#                 predictions = sess.run([top_k_op])\n",
    "#                 true_count += np.sum(predictions)\n",
    "#                 step += 1\n",
    "\n",
    "#             # Compute precision @ 1.\n",
    "#             precision = true_count / total_sample_count\n",
    "#             print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "#             summary = tf.Summary()\n",
    "#             summary.ParseFromString(sess.run(summary_op))\n",
    "#             summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "#             summary_writer.add_summary(summary, global_step)\n",
    "#         except Exception as e:  # pylint: disable=broad-except\n",
    "#             coord.request_stop(e)\n",
    "\n",
    "#         coord.request_stop()\n",
    "#         coord.join(threads, stop_grace_period_secs=10)\n",
    "#     while True:\n",
    "#       eval_once(saver, summary_writer, top_k_op, summary_op)\n",
    "#       if FLAGS.run_once:\n",
    "#         break\n",
    "#       time.sleep(FLAGS.eval_interval_secs)\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main(argv=None):  # pylint: disable=unused-argument\n",
    "#     evaluate()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
